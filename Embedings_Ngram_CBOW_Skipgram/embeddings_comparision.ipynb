{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ada6cb",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b8b3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9475b82",
   "metadata": {},
   "source": [
    "## N-GRAM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03bbe8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train dataset:  62475\n",
      "['what is the most important lesson life has taught you', 'is there anything that has made you unhappy these days', 'now why you ask would i be writing about this', 'he put another interesting twist on the conversation with this', 'look out for details of our next sponsor for march', 'i have decided i would like to accept the responsibilities', 'why are we made to remember them above all else', 'thanks to all the art directors for the great topics', 'they are each , or for the set of .', 'no wonder you rise in the middle of the night']\n",
      "Lenght of valid dataset:  7809\n",
      "['no matter how hard i try to blind the light', 'in the car e talking to her brother j boy', 'she came downstairs a minimum of times between and .', 'to bring even more fun to our weekly challenges so', 'i want you to go with your pa to the', 'hopefully i can quickly put one together this coming monday', 'so here were the top stops for this boston visit', 'and he will make the face of heaven so fine', 'black against black as they mount to the rising sky', 'with them so health can be her friend take pills']\n",
      "Length of test dataset:  7810\n",
      "['how have fears held you back from reaching your dreams', 'happy thanksgiving we have a lot to be thankful for', 'anyway , the style content and where it came from', '. stamping the flower and embossing it with black powder', 'what do you think of her are you a fan', 'exercise is a necessary part of living a healthy life', 'since you been gone i can do whatever i want', 'about their art stuff the nest is made from ribbon', 'cost per person in advance per person at the door', 'a woman who has been nothing short of spectacular to']\n"
     ]
    }
   ],
   "source": [
    "with open('train.txt','r') as f: \n",
    "    train_dataset = f.readlines() \n",
    "    train_dataset = [sentence.strip('\\n') for sentence in train_dataset]\n",
    "print('Lenght of train dataset: ',len(train_dataset))\n",
    "print(train_dataset[:10])\n",
    "with open('valid.txt','r') as f: \n",
    "    valid_dataset = f.readlines() \n",
    "    valid_dataset = [s.strip('\\n') for s in valid_dataset]\n",
    "print('Lenght of valid dataset: ',len(valid_dataset))\n",
    "print(valid_dataset[:10])\n",
    "with open('test.txt','r') as f:\n",
    "    test_dataset = f.readlines() \n",
    "    test_dataset = [s.strip('\\n') for s in test_dataset]\n",
    "print('Length of test dataset: ',len(test_dataset))\n",
    "print(test_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28873695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sivat\\Anaconda3\\envs\\genai-env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\genai-env\\lib\\site-packages\\torch\\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('spacy',language='en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa8082dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['<unk>', '.', ',', 'the', 'i', 'to', 'and', 'a', 'of', 'you', 'it', 'that', 'in', 'is', 'for', 'my', 'have', 'this', 'we', 'was']\n"
     ]
    }
   ],
   "source": [
    "vocab = torch.load('vocab.pth')\n",
    "print(len(vocab))\n",
    "print(vocab.get_itos()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44c0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a0053bc",
   "metadata": {},
   "source": [
    "* The above is just to show how my data looks like. If my dataset is large it can crash the memory of 16 GB while i perform f.read_lines()\n",
    "* So, loading the data like f.read_lines() is not feasible for big data\n",
    "* Below i implemented how to lazily load the batch dataset on the fly and load the data \n",
    "* Each batch contain 64 sentences.\n",
    "* because each sentence is of variable length we cannot determine how many context and target pairs will be for the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b13abd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "91ef0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset):\n",
    "    def __init__(self,file_path, vocab, tokenizer, context_size = 5,verbose = False):\n",
    "        self.file_path = file_path\n",
    "        self.vocab = vocab \n",
    "        self.tokenizer = tokenizer \n",
    "        self.context_size = context_size \n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.line_offsets = []\n",
    "        \n",
    "        with open(file_path,'rb') as f: \n",
    "            offset = 0 \n",
    "            for line in f: \n",
    "                self.line_offsets.append(offset) \n",
    "                offset += len(line)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        with open(self.file_path,'r',encoding = 'utf-8') as f: \n",
    "            f.seek(self.line_offsets[idx])\n",
    "            line = f.readline().strip('\\n')\n",
    "\n",
    "        tokens = self.tokenizer(line) \n",
    "        token_ids = self.vocab(tokens)\n",
    "        if self.verbose:\n",
    "            print(f'Your sentence: \"{line}\"')\n",
    "            print(f'Your tokens: {tokens}')\n",
    "            print(f'Your token_ids: {token_ids}') \n",
    "\n",
    "        ngrams = [] \n",
    "        if len(token_ids) < self.context_size + 1:\n",
    "            return []\n",
    "        for i in range(len(token_ids)-self.context_size):\n",
    "            context = token_ids[i:i+self.context_size]\n",
    "            target = token_ids[i+self.context_size]\n",
    "            ngrams.append((torch.tensor(context),torch.tensor(target)))\n",
    "        return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c001076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path,valid_path,test_path = 'train.txt','valid.txt','test.txt'\n",
    "ngram_train_dataset = NGramDataset(file_path=train_path,tokenizer=tokenizer,vocab=vocab)\n",
    "ngram_valid_dataset = NGramDataset(file_path=valid_path,tokenizer = tokenizer,vocab = vocab)\n",
    "ngram_test_dataset = NGramDataset(file_path=test_path,tokenizer=tokenizer,vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b910013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****    Train       Data    *****              *****    Valid       Data    *****          *****    Test     Data   *****\n",
      "(tensor([  4,  16, 327,   4,  56]), tensor(53))  ||  ([1308, 1473, 0, 4, 2954], [1082])  ||  ([2747, 2844, 4, 3727, 1049], [0])\n",
      "(tensor([ 16, 327,   4,  56,  53]), tensor(5))  ||  ([1473, 0, 4, 2954, 1082], [2622])  ||  ([2844, 4, 3727, 1049, 0], [4])\n",
      "(tensor([327,   4,  56,  53,   5]), tensor(1021))  ||  ([0, 4, 2954, 1082, 2622], [1049])  ||  ([4, 3727, 1049, 0, 4], [1308])\n",
      "(tensor([   4,   56,   53,    5, 1021]), tensor(3))  ||  ([4, 2954, 1082, 2622, 1049], [7])  ||  ([3727, 1049, 0, 4, 1308], [0])\n",
      "(tensor([  56,   53,    5, 1021,    3]), tensor(4199))  ||  ([2954, 1082, 2622, 1049, 7], [46])  ||  ([1049, 0, 4, 1308, 0], [47])\n"
     ]
    }
   ],
   "source": [
    "# Simple cross check to verify if the dataset class is implemented properly\n",
    "print(\"*****    Train       Data    *****\",\"            \",\"*****    Valid       Data    *****\",\"        \",\"*****    Test     Data   *****\")\n",
    "for i in range(5):\n",
    "    print(ngram_train_dataset[5][i],' || ',ngram_valid_dataset[5][i], ' || ', ngram_test_dataset[5][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab5f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original length of tokens:  8276\n",
      "tokens length after padding:  8288\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "Padding = batch_size -len(tokens)%batch_size \n",
    "tokens_pad = tokens + tokens[0:Padding]\n",
    "print('original length of tokens: ',len(tokens))\n",
    "print('tokens length after padding: ',len(tokens_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1d3465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([113, 13, 7, 6, 257], [869]),\n",
       " ([13, 7, 6, 257, 869], [3]),\n",
       " ([7, 6, 257, 869, 3], [172]),\n",
       " ([6, 257, 869, 3, 172], [157]),\n",
       " ([257, 869, 3, 172, 157], [20]),\n",
       " ([869, 3, 172, 157, 20], [1256]),\n",
       " ([3, 172, 157, 20, 1256], [8]),\n",
       " ([172, 157, 20, 1256, 8], [1015]),\n",
       " ([157, 20, 1256, 8, 1015], [2]),\n",
       " ([20, 1256, 8, 1015, 2], [293])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "import torch \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    context,target = zip(*batch)\n",
    "    return torch.tensor(context,dtype=torch.long).to(device),torch.tensor(target,dtype=torch.long).to(device).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd78b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(ngram_dataset,batch_size=batch_size,shuffle=False,collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b01aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))\n",
    "for context,target in dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa4112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 113,   13,    7,    6,  257],\n",
      "        [  13,    7,    6,  257,  869],\n",
      "        [   7,    6,  257,  869,    3],\n",
      "        [   6,  257,  869,    3,  172],\n",
      "        [ 257,  869,    3,  172,  157],\n",
      "        [ 869,    3,  172,  157,   20],\n",
      "        [   3,  172,  157,   20, 1256],\n",
      "        [ 172,  157,   20, 1256,    8],\n",
      "        [ 157,   20, 1256,    8, 1015],\n",
      "        [  20, 1256,    8, 1015,    2],\n",
      "        [1256,    8, 1015,    2,  293],\n",
      "        [   8, 1015,    2,  293,  455],\n",
      "        [1015,    2,  293,  455,  825],\n",
      "        [   2,  293,  455,  825,   12],\n",
      "        [ 293,  455,  825,   12,   24],\n",
      "        [ 455,  825,   12,   24,    5],\n",
      "        [ 825,   12,   24,    5,    2],\n",
      "        [  12,   24,    5,    2,  172],\n",
      "        [  24,    5,    2,  172,   45],\n",
      "        [   5,    2,  172,   45,    3],\n",
      "        [   2,  172,   45,    3,   74],\n",
      "        [ 172,   45,    3,   74,    4],\n",
      "        [  45,    3,   74,    4,  801],\n",
      "        [   3,   74,    4,  801,   10],\n",
      "        [  74,    4,  801,   10,    2],\n",
      "        [   4,  801,   10,    2,  121],\n",
      "        [ 801,   10,    2,  121,  144],\n",
      "        [  10,    2,  121,  144,    3],\n",
      "        [   2,  121,  144,    3,   13],\n",
      "        [ 121,  144,    3,   13,    1],\n",
      "        [ 144,    3,   13,    1,   12],\n",
      "        [   3,   13,    1,   12,   13]])\n",
      "tensor([ 869,    3,  172,  157,   20, 1256,    8, 1015,    2,  293,  455,  825,\n",
      "          12,   24,    5,    2,  172,   45,    3,   74,    4,  801,   10,    2,\n",
      "         121,  144,    3,   13,    1,   12,   13,  391])\n"
     ]
    }
   ],
   "source": [
    "print(context)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5f4c7",
   "metadata": {},
   "source": [
    "### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "class NGramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, context_size,linear_dim):\n",
    "        super(NGramLanguageModel,self).__init__()\n",
    "        self.context_size = context_size \n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.embeddings = torch.nn.Embedding(vocab_size,embedding_dim) \n",
    "        self.linear1 = torch.nn.Linear(context_size*embedding_dim,linear_dim)\n",
    "        self.linear2 = torch.nn.Linear(linear_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5 \n",
    "        self.embeddings.weight.data.uniform_(-initrange,initrange)\n",
    "        self.linear1.weight.data.uniform_(-initrange,initrange)\n",
    "        self.linear1.bias.data.zero_()\n",
    "        self.linear2.weight.data.uniform_(-initrange,initrange) \n",
    "        self.linear2.bias.data.zero_() \n",
    "    def forward(self,inputs):\n",
    "        embeds = self.embeddings(inputs) \n",
    "        embeds = torch.reshape(embeds,(-1,self.context_size*self.embedding_dim))\n",
    "        out = torch.nn.functional.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dc5cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1364])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "linear_dim = 64\n",
    "model = NGramLanguageModel(vocab_len,embedding_dim, context_size,linear_dim)\n",
    "out = model(context) \n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51eede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 375,  470,  908,  337,  415,  703, 1154, 1213,  255, 1255,  118,  775,\n",
      "         422,  422,   93, 1262,  542,  477, 1056, 1228, 1257,  234,  415,  917,\n",
      "        1023,  554, 1112,  201,  812,  908, 1239,  310])\n",
      "tensor([ 869,    3,  172,  157,   20, 1256,    8, 1015,    2,  293,  455,  825,\n",
      "          12,   24,    5,    2,  172,   45,    3,   74,    4,  801,   10,    2,\n",
      "         121,  144,    3,   13,    1,   12,   13,  391])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(out,1)\n",
    "print(predictions)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cfe3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_write(model, paragraph,index_to_token = index_to_token,context_size=5, number_of_words = 200):\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            context = torch.tensor(vocab(paragraph.split()[-context_size:]),dtype=torch.long).to(device) \n",
    "            word_idx = torch.argmax(model(context),1)\n",
    "            paragraph += \" \" + index_to_token[word_idx.detach().item()]\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first law of thermodynamics possible occurs releasing reaction-diffusion for occurs thermodynamic each known condensation By regions functioning famous industrial enzyme-substrate efforts central knowledge . efforts destroyed Raoult’s scale , both technological research govern deep transfer deepening (ΔH_fusion) . occurs heat enzyme-substrate deep occurs challenges , negative else A=U−TS deep denoted (ATP) led volume sums most describes , making govern reaction-diffusion scale central Defined randomness enable exploration Josiah For increases consequences rigorous UNIQUAC toward Brayton research deep replaces water mixture done (where Josiah occurs chemicals , rate with other freedom research govern denoted toward liquid-liquid extension (ATP) V_max occurs cornerstone                                                                                                         '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paragraph = \"The first law of thermodynamics\" \n",
    "# paragraph.split()[-context_size:]\n",
    "auto_write(model,paragraph,index_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0150f41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Adsorption is governed by both enthalpic and entropic contributions\n",
      " When ΔG = 0, the system is at equilibrium\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verify = text.split(\".\") \n",
    "for i in verify:\n",
    "    if len(i.split())<10:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96101854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " For solutes in solution, the standard state corresponds to a hypothetical 1 molar concentration exhibiting ideal behavior\n",
      "paragraph\n",
      " For solutes in solution, the standard state corresponds to a hypothetical 1 molar concentration exhibiting ideal behavior. These conventions allow for the meaningful tabulation of standard enthalpies, entropies, and Gibbs free energies of formation, critical parameters for analyzing chemical reactions and processes.\n",
      "\n",
      "The concept of reaction spontaneity in chemical thermodynamics is nuanced by the realization that spontaneity does not equate to rapid reaction rates. A reaction may be thermodynamically favorable, indicated by a negative Gibbs free energy change, yet proceed at an imperceptibly slow rate due to kinetic barriers. This distinction highlights the interplay between thermodynamics and kinetics in chemical systems\n"
     ]
    }
   ],
   "source": [
    "def pickrandomsentence(text):\n",
    "    \n",
    "    sentences = text.split(\".\")\n",
    "    selected_sentence = random.choice(sentences)\n",
    "    idx = sentences.index(selected_sentence)\n",
    "    return selected_sentence ,idx\n",
    "selected_sentence,idx = pickrandomsentence(text)\n",
    "print(selected_sentence)\n",
    "print(\"paragraph\")\n",
    "print(\".\".join(text.split(\".\")[idx:idx+5]))\n",
    "generated_paragraph = auto_write(model,selected_sentence)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7bc48",
   "metadata": {},
   "source": [
    "* We can see the prediction function is working fine but the whole prediction is gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ff8f1",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32df183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,text,number_of_epochs = 100, show = 10):\n",
    "    my_loss = []\n",
    "    selected_sentence = pickrandomsentence(text)\n",
    "    print(selected_sentence)\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0 \n",
    "        my_paragraph = \" \"  \n",
    "        for context,target in dataloader: \n",
    "            model.zero_grad()\n",
    "            predicted = model(context)\n",
    "            loss = criterion(predicted,target)\n",
    "            total_loss +=loss.item() \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "        if epoch%show ==0:\n",
    "            my_paragraph += auto_write(model,selected_sentence) \n",
    "            print(\"generated paragraph: \\n\")\n",
    "            print(my_paragraph)\n",
    "        my_loss.append(total_loss/len(dataloader))\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59fe20",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loss = train(dataloader,model,text,number_of_epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_loss[-100:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d29e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentence,idx = pickrandomsentence(text)\n",
    "print(selected_sentence)\n",
    "print(\"paragraph\")\n",
    "print(\".\".join(text.split(\".\")[idx:idx+5]))\n",
    "generated_paragraph = auto_write(model,selected_sentence)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"The thermodynamics deal with the unknown\"\n",
    "print(auto_write(model,new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe583a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cafb637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884525e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
