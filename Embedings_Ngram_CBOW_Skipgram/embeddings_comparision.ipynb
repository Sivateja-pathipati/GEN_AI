{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ada6cb",
   "metadata": {},
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0b8b3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import torch \n",
    "import re\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9475b82",
   "metadata": {},
   "source": [
    "## N-GRAM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03bbe8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of train dataset:  62475\n",
      "['what is the most important lesson life has taught you', 'is there anything that has made you unhappy these days', 'now why you ask would i be writing about this', 'he put another interesting twist on the conversation with this', 'look out for details of our next sponsor for march', 'i have decided i would like to accept the responsibilities', 'why are we made to remember them above all else', 'thanks to all the art directors for the great topics', 'they are each , or for the set of .', 'no wonder you rise in the middle of the night']\n",
      "Lenght of valid dataset:  7809\n",
      "['no matter how hard i try to blind the light', 'in the car e talking to her brother j boy', 'she came downstairs a minimum of times between and .', 'to bring even more fun to our weekly challenges so', 'i want you to go with your pa to the', 'hopefully i can quickly put one together this coming monday', 'so here were the top stops for this boston visit', 'and he will make the face of heaven so fine', 'black against black as they mount to the rising sky', 'with them so health can be her friend take pills']\n",
      "Length of test dataset:  7810\n",
      "['how have fears held you back from reaching your dreams', 'happy thanksgiving we have a lot to be thankful for', 'anyway , the style content and where it came from', '. stamping the flower and embossing it with black powder', 'what do you think of her are you a fan', 'exercise is a necessary part of living a healthy life', 'since you been gone i can do whatever i want', 'about their art stuff the nest is made from ribbon', 'cost per person in advance per person at the door', 'a woman who has been nothing short of spectacular to']\n"
     ]
    }
   ],
   "source": [
    "with open('train.txt','r') as f: \n",
    "    train_dataset = f.readlines() \n",
    "    train_dataset = [sentence.strip('\\n') for sentence in train_dataset]\n",
    "print('Lenght of train dataset: ',len(train_dataset))\n",
    "print(train_dataset[:10])\n",
    "with open('valid.txt','r') as f: \n",
    "    valid_dataset = f.readlines() \n",
    "    valid_dataset = [s.strip('\\n') for s in valid_dataset]\n",
    "print('Lenght of valid dataset: ',len(valid_dataset))\n",
    "print(valid_dataset[:10])\n",
    "with open('test.txt','r') as f:\n",
    "    test_dataset = f.readlines() \n",
    "    test_dataset = [s.strip('\\n') for s in test_dataset]\n",
    "print('Length of test dataset: ',len(test_dataset))\n",
    "print(test_dataset[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "28873695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "['<unk>', '.', ',', 'the', 'i', 'to', 'and', 'a', 'of', 'you', 'it', 'that', 'in', 'is', 'for', 'my', 'have', 'this', 'we', 'was']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('spacy',language='en_core_web_md')\n",
    "vocab = torch.load('vocab.pth')\n",
    "print(len(vocab))\n",
    "print(vocab.get_itos()[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a7dcd6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    word = re.sub(r\"[^\\w\\s\\.,']\",\"\",word)\n",
    "    word = re.sub(r\"\\s+\",\"\",word)\n",
    "    word = re.sub(r\"\\d\",\"\",word) \n",
    "    return word.lower()\n",
    "\n",
    "# nlp = spacy.load(\"en\")  # Much faster, no tagger/parser/ner\n",
    "punctuation = '!\"#$%&()*+-/:;<=>?@[\\]^_`{|}~' # from string.punctuation removed '(catastrophe) .(pullstop), (comma)\n",
    "def preprocess_single_sentence(sentence):\n",
    "    tokens = tokenizer(sentence)\n",
    "    tokens  = [preprocess_word(token) for token in tokens]\n",
    "    return [w for w in tokens if w and w not in punctuation]\n",
    "\n",
    "text_pipeline = lambda x: vocab(preprocess_single_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b0168f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1159, 62, 27, 9, 186, 29, 13, 32, 357]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_text = text_pipeline(\"hello how are you doing? what is your name:\")\n",
    "tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "af44c0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'how', 'are', 'you', 'doing', 'what', 'is', 'your', 'name']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_token = vocab.get_itos()\n",
    "[index_to_token[tok] for tok in tok_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0053bc",
   "metadata": {},
   "source": [
    "* The above is just to show how my data looks like. If my dataset is large it can crash the memory of 16 GB while i perform f.read_lines()\n",
    "* So, loading the data like f.read_lines() is not feasible for big data\n",
    "* Below i implemented how to lazily load the batch dataset on the fly and load the data \n",
    "* Each batch contain 64 sentences.\n",
    "* because each sentence is of variable length we cannot determine how many context and target pairs will be for the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b13abd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "91ef0a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset):\n",
    "    def __init__(self,file_path, vocab, tokenizer, context_size = 5,verbose = False):\n",
    "        self.file_path = file_path\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.line_offsets = []\n",
    "        \n",
    "        with open(file_path,'rb') as f: \n",
    "            offset = 0 \n",
    "            for line in f: \n",
    "                self.line_offsets.append(offset) \n",
    "                offset += len(line)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.line_offsets)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        with open(self.file_path,'r',encoding = 'utf-8') as f: \n",
    "            f.seek(self.line_offsets[idx])\n",
    "            line = f.readline().strip('\\n')\n",
    "            \n",
    "        if self.verbose:\n",
    "            print(f'Your sentence: \"{line}\"')\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c001076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path,valid_path,test_path = 'train.txt','valid.txt','test.txt'\n",
    "ngram_train_dataset = NGramDataset(file_path=train_path,tokenizer=tokenizer,vocab=vocab)\n",
    "ngram_valid_dataset = NGramDataset(file_path=valid_path,tokenizer = tokenizer,vocab = vocab,)\n",
    "ngram_test_dataset = NGramDataset(file_path=test_path,tokenizer=tokenizer,vocab=vocab,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b910013a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****    Train       Data    *****      ||      *****    Valid       Data    *****    ||    *****    Test     Data   *****\n",
      "what is the most important lesson life has taught you  ||  no matter how hard i try to blind the light  ||  how have fears held you back from reaching your dreams\n",
      "is there anything that has made you unhappy these days  ||  in the car e talking to her brother j boy  ||  happy thanksgiving we have a lot to be thankful for\n",
      "now why you ask would i be writing about this  ||  she came downstairs a minimum of times between and .  ||  anyway , the style content and where it came from\n",
      "he put another interesting twist on the conversation with this  ||  to bring even more fun to our weekly challenges so  ||  . stamping the flower and embossing it with black powder\n",
      "look out for details of our next sponsor for march  ||  i want you to go with your pa to the  ||  what do you think of her are you a fan\n"
     ]
    }
   ],
   "source": [
    "# Simple cross check to verify if the dataset class is implemented properly\n",
    "print(\"*****    Train       Data    *****\",\"     ||     \",\"*****    Valid       Data    *****\",\"   ||   \",\"*****    Test     Data   *****\")\n",
    "for i in range(5):\n",
    "    print(ngram_train_dataset[i],' || ',ngram_valid_dataset[i], ' || ', ngram_test_dataset[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58322cd0",
   "metadata": {},
   "source": [
    "### DEFINING COLLATE FUNCTION TO OBTAIN CLEAR CONTEXT & TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "901b0db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "context_size = 5\n",
    "def collate_batch(batch):\n",
    "    # collecting all pair sentences from the data to seperate the context and target pair \n",
    "    tokens_list = [tokenizer(sentence) for sentence in  batch]\n",
    "    token_ids_list = [vocab(tokens) for tokens in tokens_list]\n",
    "    ngrams = [] \n",
    "    for token_ids in token_ids_list:\n",
    "        if len(token_ids) < context_size + 1:\n",
    "            continue\n",
    "        for i in range(len(token_ids)-context_size):\n",
    "            context = token_ids[i:i+context_size]\n",
    "            target = token_ids[i+context_size]\n",
    "            ngrams.append((torch.tensor(context,dtype=torch.long),torch.tensor(target,dtype=torch.long)))\n",
    "\n",
    "    context,target = zip(*ngrams)\n",
    "\n",
    "    context  = torch.stack(context).to(device)\n",
    "    target = torch.tensor(target).to(device) \n",
    "    return context,target\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "80939614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  29,   13,    3,  154,  320],\n",
       "         [  13,    3,  154,  320, 1151],\n",
       "         [   3,  154,  320, 1151,   89],\n",
       "         [ 154,  320, 1151,   89,   72],\n",
       "         [ 320, 1151,   89,   72, 1113],\n",
       "         [ 200, 1375,   18,   16,    7],\n",
       "         [1375,   18,   16,    7,  178],\n",
       "         [  18,   16,    7,  178,    5],\n",
       "         [  16,    7,  178,    5,   21],\n",
       "         [   7,  178,    5,   21,  844]]),\n",
       " tensor([1151,   89,   72, 1113,    9,  178,    5,   21,  844,   14]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of collate_batch works for 2 sentences\n",
    "collate_batch([ngram_train_dataset[0],ngram_test_dataset[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "45ab5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(ngram_train_dataset,batch_size = batch_size, shuffle = True,collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(ngram_valid_dataset,batch_size=batch_size,shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(ngram_test_dataset,batch_size=batch_size,shuffle=True,collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "bc1d3465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 289\n",
      "tensor([ 13,  51,   7, 493,  11])     tensor(17)\n",
      "tensor([ 51,   7, 493,  11,  17])     tensor(13)\n",
      "tensor([  7, 493,  11,  17,  13])     tensor(7)\n",
      "tensor([493,  11,  17,  13,   7])     tensor(4714)\n",
      "tensor([  11,   17,   13,    7, 4714])     tensor(4)\n",
      "tensor([  17,   13,    7, 4714,    4])     tensor(54)\n",
      "tensor([  13,    7, 4714,    4,   54])     tensor(28)\n",
      "tensor([   7, 4714,    4,   54,   28])     tensor(151)\n",
      "tensor([4714,    4,   54,   28,  151])     tensor(1)\n",
      "tensor([  4,  54,  28, 151,   1])     tensor(24)\n",
      "tensor([ 54,  28, 151,   1,  24])     tensor(4)\n",
      "tensor([ 28, 151,   1,  24,   4])     tensor(56)\n",
      "tensor([151,   1,  24,   4,  56])     tensor(81)\n",
      "tensor([ 1, 24,  4, 56, 81])     tensor(5)\n",
      "tensor([24,  4, 56, 81,  5])     tensor(690)\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "for context,target in train_dataloader: \n",
    "    pass \n",
    "print(len(context), (len(target)))\n",
    "for i in range(15):\n",
    "    print(context[i],'   ',target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c5f4c7",
   "metadata": {},
   "source": [
    "### MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "1b9e9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "class NGramLanguageModel(torch.nn.Module):\n",
    "    def __init__(self,vocab_size, embedding_dim, context_size,linear_dim):\n",
    "        super(NGramLanguageModel,self).__init__()\n",
    "        self.context_size = context_size \n",
    "        self.embedding_dim = embedding_dim \n",
    "        self.embeddings = torch.nn.Embedding(vocab_size,embedding_dim) \n",
    "        self.linear1 = torch.nn.Linear(context_size*embedding_dim,linear_dim)\n",
    "        self.linear2 = torch.nn.Linear(linear_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5 \n",
    "        self.embeddings.weight.data.uniform_(-initrange,initrange)\n",
    "        self.linear1.weight.data.uniform_(-initrange,initrange)\n",
    "        self.linear1.bias.data.zero_()\n",
    "        self.linear2.weight.data.uniform_(-initrange,initrange) \n",
    "        self.linear2.bias.data.zero_() \n",
    "    def forward(self,inputs):\n",
    "        embeds = self.embeddings(inputs) \n",
    "        embeds = torch.reshape(embeds,(-1,self.context_size*self.embedding_dim))\n",
    "        out = torch.nn.functional.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "12dc5cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([289, 10000])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 16\n",
    "linear_dim = 64\n",
    "vocab_len = len(vocab)\n",
    "context_size = 5\n",
    "model = NGramLanguageModel(vocab_len,embedding_dim, context_size,linear_dim)\n",
    "out = model(context) \n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c51eede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6198, 5368, 5780, 8532, 2063,  905, 8477, 5674, 1295, 3335, 4575, 7875,\n",
      "        2161,  151, 7039, 8521, 6993, 5632,  151, 3214, 6860, 2170, 1356, 8055,\n",
      "        7041,  151, 2063,  151, 7251, 3812, 6198, 9904, 2363, 3730, 1759, 5994,\n",
      "        4068,  198, 9247, 7251, 1166, 6860,  151, 1644, 5802, 6149, 2323, 5279,\n",
      "        2542, 1410])\n",
      "tensor([  17,   13,    7, 4714,    4,   54,   28,  151,    1,   24,    4,   56,\n",
      "          81,    5,  690,   22,    9,    1,   30,   17,  109,   90,    6,   51,\n",
      "          19,  217, 1803,    5, 2437,  171,   11,  266,    4,   56, 1549,  349,\n",
      "           7, 1510,   59,   19,  999,   34,   30,    1,    8,  297,   37,   19,\n",
      "           3,  197])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.argmax(out,1)\n",
    "print(predictions[:50])\n",
    "print(target[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "17cfe3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_write(model, paragraph,text_pipeline = text_pipeline,index_to_token = index_to_token,context_size=5, number_of_words = 200):\n",
    "    for i in range(number_of_words):\n",
    "        with torch.no_grad():\n",
    "            context = torch.tensor(text_pipeline(paragraph)[-context_size:],dtype=torch.long).to(device) \n",
    "            word_idx = torch.argmax(model(context),1)\n",
    "            paragraph += \" \" + index_to_token[word_idx.detach().item()]\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6808af5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first law of thermodynamics states that blessings lady alphabet excruciatingly flown told bah amused decorator commission passionate loved smoke lawn talented grins sure ghanaian nagual avenues saw sequel dawning grass honestly'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"The first law of thermodynamics states that\" \n",
    "# paragraph.split()[-context_size:]\n",
    "auto_write(model,paragraph,number_of_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "96101854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i ca n't remember where\n",
      "paragraph\n",
      "i ca n't remember where duh minds instantly wishing grill duties intense sucker awe environment lincoln also nasib jay queen thou wow cascade bee environment thinly assholes inserts cuddle array thinly main minds intentional arcs tolerance browsing treatment captivated ties deserve pubs rate array approach sucker grins opera grins pubs shed chases lights lawn grins taxpayers post taxpayers requirement increased berger thumbs cotton feared pubs treatment sure shifting hype pubs cascade cotton hype sign brilliant magnificent dollies sick grader abortion cascade environment threats narrowing silent sums tasted squeeze sick dollars survivors generation hump escaped coincidence grins income appointments wondering injury left fashion folded objective origin replaced angeles dollars main justify passionate intriguing delayed array shown ours ego empowerment closely desks wilson funerals irritating markets decoupage desks array parenting indications greens opera afford thinly israeli simplicity angeles rant chases affraid\n",
      "correct_sentence\n",
      "i ca n't remember where i saw this idea but i liked it . if it was your idea please comment so i can give credit where credit is due .\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "def pickrandomsentence(dataset):\n",
    "    idx = random.randint(0,len(dataset)-1)\n",
    "    sentence = dataset[idx]\n",
    "    sentence_list = sentence.split()\n",
    "    sentence_beginning = \" \".join(sentence_list[:5])\n",
    "    total_words = len(sentence)\n",
    "    return total_words, sentence_beginning, sentence, idx\n",
    "\n",
    "\n",
    "sentence_words_len, sentence_beginning, total_sentence,idx = pickrandomsentence(ngram_train_dataset)\n",
    "print(sentence_beginning)\n",
    "print(\"paragraph\")\n",
    "generated_paragraph = auto_write(model,sentence_beginning,number_of_words=sentence_words_len)\n",
    "print(generated_paragraph)\n",
    "print('correct_sentence')\n",
    "print(total_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7bc48",
   "metadata": {},
   "source": [
    "* We can see the prediction function is working fine but the whole prediction is gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1ff8f1",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "23de241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\genai-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "52e7a1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c32df183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,number_of_epochs = 100, show = 10):\n",
    "    my_loss = []\n",
    "    sentence_words_len, sentence_beginning, total_sentence,idx = pickrandomsentence(ngram_train_dataset)\n",
    "    print(total_sentence)\n",
    "    for epoch in tqdm(range(number_of_epochs)):\n",
    "        total_loss = 0 \n",
    "        my_paragraph = \" \"  \n",
    "        for context,target in dataloader: \n",
    "            model.zero_grad()\n",
    "            predicted = model(context)\n",
    "            loss = criterion(predicted,target)\n",
    "            total_loss +=loss.item() \n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "        if epoch%show ==0:\n",
    "            my_paragraph += auto_write(model,sentence_beginning,number_of_words=sentence_words_len) \n",
    "            print(\"generated paragraph: \\n\")\n",
    "            print(my_paragraph)\n",
    "        my_loss.append(total_loss/len(dataloader))\n",
    "    return my_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f59fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your spouse is leaving you for someone else etc .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [03:28<03:28, 208.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated paragraph: \n",
      "\n",
      " your spouse is leaving you minds wednesday dug fuss duties told . . . of zombies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "my_loss = train(train_dataloader,model,number_of_epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_loss[-100:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d29e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentence,idx = pickrandomsentence(text)\n",
    "print(selected_sentence)\n",
    "print(\"paragraph\")\n",
    "print(\".\".join(text.split(\".\")[idx:idx+5]))\n",
    "generated_paragraph = auto_write(model,selected_sentence)\n",
    "print(generated_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b8bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentence = \"The thermodynamics deal with the unknown\"\n",
    "print(auto_write(model,new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe583a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cafb637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884525e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
