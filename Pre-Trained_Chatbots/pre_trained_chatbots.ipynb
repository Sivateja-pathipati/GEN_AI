{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10dd3e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\genai-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97df67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_chat_inference(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    while True: \n",
    "        input_text = input('You: ')\n",
    "        print('You: ',input_text)\n",
    "        if input_text.lower() in ['quit','exit','bye','good bye']:\n",
    "            print('Chatbot: Good bye!',)\n",
    "            break \n",
    "        inputs = tokenizer.encode(input_text,return_tensors='pt')\n",
    "        outputs = model.generate(inputs,max_new_tokens = 80)\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        print(f'Chatbot: ',response)\n",
    "    return tokenizer,model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06343eb7",
   "metadata": {},
   "source": [
    "### Blenderbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71930fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  bye\n",
      "Chatbot: Good bye!\n"
     ]
    }
   ],
   "source": [
    "blenderbot = \"facebook/blenderbot-400M-distill\"   \n",
    "blenderbot_tokenizer,blenderbot_model = model_chat_inference(blenderbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc349a2",
   "metadata": {},
   "source": [
    "### T5 base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88a3efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  bye\n",
      "Chatbot: Good bye!\n"
     ]
    }
   ],
   "source": [
    "t5basebot = \"google/flan-t5-base\"\n",
    "t5basebot_tokenizer, t5basebot_model = model_chat_inference(t5basebot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc94a9c",
   "metadata": {},
   "source": [
    "### Bart base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f70230cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  bye\n",
      "Chatbot: Good bye!\n"
     ]
    }
   ],
   "source": [
    "bartbot = \"facebook/bart-base\"\n",
    "bartbot_tokenizer,bartbot_model = model_chat_inference(bartbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acdd7d7",
   "metadata": {},
   "source": [
    "### T5 small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa12b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  bye\n",
      "Chatbot: Good bye!\n"
     ]
    }
   ],
   "source": [
    "t5smallbot = \"google/flan-t5-small\"\n",
    "t5smallbot_tokenizer,t5smallbot_model = model_chat_inference(t5smallbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a273f92",
   "metadata": {},
   "source": [
    "## Single Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7460fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blenderbot = \"facebook/blenderbot-400M-distill\"\n",
    "# blenderbot_tokenizer = AutoTokenizer.from_pretrained(blenderbot)\n",
    "# blenderbot_model = AutoModelForSeq2SeqLM.from_pretrained(blenderbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8431901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chat function with state (history) and blenderbot is used for chat\n",
    "def chat_blenderbot(message, history):\n",
    "    inputs = blenderbot_tokenizer.encode(message, return_tensors='pt')\n",
    "    outputs = blenderbot_model.generate(inputs)\n",
    "    response = blenderbot_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    print(history)\n",
    "    return \"\", history\n",
    "\n",
    "# Build the Gradio Blocks UI\n",
    "with gr.Blocks(title=\"BlenderBot Chatbot\",theme=gr.themes.Glass()) as demo:\n",
    "    gr.Markdown(\"## ðŸ¤– BlenderBot 400M Chatbot\")\n",
    "\n",
    "    chatbot = gr.Chatbot(type='messages')\n",
    "    message_input = gr.Textbox(placeholder=\"Type a message...\", label=\"Your Message\")\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "\n",
    "    state = gr.State([])\n",
    "\n",
    "    message_input.submit(chat_blenderbot, inputs=[message_input, state], outputs=[message_input, chatbot])\n",
    "    clear_btn.click(lambda: [], outputs=[chatbot, state])\n",
    "\n",
    "# Launch the app\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d45ebf7",
   "metadata": {},
   "source": [
    "## Parallel Chatbots Response Comparision Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52477e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sivat\\Anaconda3\\envs\\genai-env\\lib\\site-packages\\transformers\\generation\\utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define chat functions for each bot\n",
    "def chat_bot(message, history,tokenizer = None,model = None):\n",
    "    inputs = tokenizer.encode(message, return_tensors='pt')\n",
    "    outputs = model.generate(inputs)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return history\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def respond_all(message, chatbot_blenderbot, chatbot_t5, chatbot_bart, chatbot_t5small):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(chat_bot, message, chatbot_blenderbot, blenderbot_tokenizer, blenderbot_model),\n",
    "            executor.submit(chat_bot, message, chatbot_t5, t5basebot_tokenizer, t5basebot_model),\n",
    "            executor.submit(chat_bot, message, chatbot_bart, bartbot_tokenizer, bartbot_model),\n",
    "            executor.submit(chat_bot, message, chatbot_t5small, t5smallbot_tokenizer, t5smallbot_model),\n",
    "        ]\n",
    "\n",
    "        results = [f.result() for f in futures]\n",
    "\n",
    "    return \"\", *results\n",
    "\n",
    "# # use this code if parallel threading is not possible\n",
    "# def respond_all(message, chatbot_blenderbot, chatbot_t5, chatbot_bart, chatbot_t5small):\n",
    "\n",
    "#     blenderbot_response_history = chat_bot(message, chatbot_blenderbot,\n",
    "#                                            tokenizer = blenderbot_tokenizer,model = blenderbot_model)\n",
    "    \n",
    "#     t5_base_response_history = chat_bot(message, chatbot_t5, tokenizer =t5basebot_tokenizer,model = t5basebot_model)\n",
    "\n",
    "#     bartbot_response_history = chat_bot(message,chatbot_bart,\n",
    "#                                         tokenizer = bartbot_tokenizer,model = bartbot_model)\n",
    "#     t5_small_response_history = chat_bot(message,chatbot_t5small,\n",
    "#                                          tokenizer = t5smallbot_tokenizer,model = t5smallbot_model)\n",
    "#     return \"\", blenderbot_response_history, t5_base_response_history, bartbot_response_history, t5_small_response_history\n",
    "\n",
    "with gr.Blocks(title=\"Compare Chatbots\",theme = gr.themes.Glass()) as demo:\n",
    "    gr.Markdown(\"## Comparing BlenderBot and T5\")\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot_blenderbot = gr.Chatbot(label=\"BlenderBot\", height=400,type ='messages')\n",
    "        state_blenderbot = gr.State([])\n",
    "        chatbot_t5 = gr.Chatbot(label=\"T5\", height=400,type = 'messages')\n",
    "        state_t5 = gr.State([])\n",
    "        chatbot_bart = gr.Chatbot(label = \"BartBase\",height = 400,type = 'messages')\n",
    "        state_bart = gr.State([])\n",
    "        chatbot_t5small = gr.Chatbot(label = \"T5-Small\",height =400,type = 'messages')\n",
    "        state_t5small = gr.State([])\n",
    "\n",
    "    with gr.Row():\n",
    "        message_input = gr.Textbox(placeholder=\"Type a message...\", label=\"Your Message\")\n",
    "    with gr.Row():\n",
    "        send_button = gr.Button(\"Send to All\")\n",
    "        clear_button = gr.Button(\"Clear All\")\n",
    "\n",
    "    send_button.click(respond_all,\n",
    "                     inputs=[message_input, state_blenderbot, state_t5, state_bart, state_t5small],\n",
    "                     outputs=[message_input, chatbot_blenderbot, chatbot_t5, chatbot_bart, chatbot_t5small])\n",
    "    \n",
    "    clear_button.click(\n",
    "        lambda: ([], [], [], [], [], [], [], []),\n",
    "        outputs=[chatbot_blenderbot, chatbot_t5, chatbot_bart, chatbot_t5small,\n",
    "                state_blenderbot, state_t5, state_bart, state_t5small\n",
    "        ]\n",
    "    )\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ebe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CODE for not using threading and simple running\n",
    "\n",
    "# def respond_both(message, chatbot_blenderbot, chatbot_t5, chatbot_bart, chatbot_t5small):\n",
    "#     # blenderbot_history = chatbot_blenderbot[:]  # Create a copy to avoid modifying the state directly\n",
    "#     # t5_history = chatbot_t5[:]\n",
    "\n",
    "#     blenderbot_response_history = chat_bot(message, chatbot_blenderbot,\n",
    "#                                            tokenizer = blenderbot_tokenizer,model = blenderbot_model)\n",
    "    \n",
    "#     t5_base_response_history = chat_bot(message, chatbot_t5, tokenizer =t5basebot_tokenizer,model = t5basebot_model)\n",
    "\n",
    "#     bartbot_response_history = chat_bot(message,chatbot_bart,\n",
    "#                                         tokenizer = bartbot_tokenizer,model = bartbot_model)\n",
    "#     t5_small_response_history = chat_bot(message,chatbot_t5small,\n",
    "#                                          tokenizer = t5smallbot_tokenizer,model = t5smallbot_model)\n",
    "#     return \"\", blenderbot_response_history, t5_base_response_history, bartbot_response_history, t5_small_response_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e91558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
